{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Runoff Events Table\n",
    "\n",
    "__Description__: Excess rainfall is calculated by first randomly selecting a precipitation recurrance interval and corresponding precipitation amount, precipitation temporal distribution, and curve number for the area of interest. The randomly selected precipitation data and curve number are then used by the curve number approach to calculate the excess rainfall amount for the corresponding recurrance interval. The procedure is repeated for the specified number of events/recurrance intervals. The incremental excess rainfall curves are grouped based on a novel test statistic that quantifies the incremental and cumulative volumentric differences between two curves. The mean of each group of curves is calculated and used in place of the original set of curves in order to improve modeling efficiency by reducing redundancy.  \n",
    "\n",
    "__Input__: \n",
    "- Parameters: curve number; the volume, region, and duration ([See map for volume and region](https://hdsc.nws.noaa.gov/hdsc/pfds/pfds_temporal.html)); and grouping parameters (tempEpsilon, volEpsilon, and convEpsilon).\n",
    "        \n",
    "- The area averaged precipitation frequency data for the specified duration.\n",
    "\n",
    "- `DataRepository` folder which contains the following:\n",
    "    - The *Temporal_Distributions* folder, containing [precipitation temporal distribution data](https://hdsc.nws.noaa.gov/hdsc/pfds/pfds_temporal.html) broken down by quartile for the specified volume, region, and duration.\n",
    "\n",
    "    - The *Temporal_Distribution_Data_Map.json* which contains metadata used to extract the temporal distribution data from its *.csv file.\n",
    "\n",
    "    - The *Temporal_Quartile_Ranks.xlsx* which contains the percentage of precipitation events whose temporal distributions are represented by those in each quartile. [Source](https://www.nws.noaa.gov/oh/hdsc/currentpf.html). \n",
    "\n",
    "    - The *NEH630_Table_10_1.json* which contains information about the spread of possible values around the provided (expected) curve number. [Source](https://www.wcc.nrcs.usda.gov/ftpref/wntsc/H&H/NEHhydrology/ch10.pdf).\n",
    "\n",
    "\n",
    "__Output__: A *.csv* file containing mean incremental excess rainfall curves and a *.json* file containing the metadata associated with the mean curves, including the original events that were grouped to produce the mean curve and the corresponding test statistics.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Load Libraries, Parameters, and Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('core')\n",
    "import hydromet_conv\n",
    "from hydromet import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Papermill (volume/region/duration dependent):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Excess rainfall parameters:\n",
    "CN = 83       # Curve number\n",
    "volume = 2    # NOAA Atlas 14 volume\n",
    "region = 1    # NOAA Atlas 14 region\n",
    "duration = 24  # Event duration in hour\n",
    "\n",
    "\n",
    "## Grouping parameters:\n",
    "tempEpsilon = 4 \n",
    "tempEpsilon2 = 2 \n",
    "convEpsilon = 100  \n",
    "volEpsilon = 67  \n",
    "\n",
    "\n",
    "## Filenames and paths:\n",
    "AOI = '020700100204'\n",
    "precip_table = 'PrecipTable_{0}.xlsx'.format(AOI) \n",
    "\n",
    "root_dir = pl.Path(os.getcwd())\n",
    "outputs_dir = root_dir/'Outputs'\n",
    "precip_table_dir = outputs_dir/precip_table\n",
    "datarepository_dir = root_dir/'DataRepository'\n",
    "bin_dir = root_dir/'bin'\n",
    "\n",
    "\n",
    "## Options:\n",
    "seed = np.random.randint(low=0, high=10000)\n",
    "papermill = False\n",
    "display_plots = True\n",
    "display_print = True\n",
    "save_dss = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert all paths to objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_table_dir = pl.Path(precip_table_dir)\n",
    "datarepository_dir = pl.Path(datarepository_dir)\n",
    "bin_dir = pl.Path(bin_dir)\n",
    "outputs_dir = pl.Path(outputs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global (project specific):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Excess rainfall parameters:\n",
    "n_events = 10000 \n",
    "lower_limit, upper_limit = 2, 3000 \n",
    "nquartiles = 4 \n",
    "\n",
    "\n",
    "## Grouping parameters:\n",
    "nbins = 50\n",
    "test_stat_threshold = 0  \n",
    "\n",
    "\n",
    "## DSS output parameters:\n",
    "variable = \"Excess-Rainfall\"\n",
    "data_type = 'INST-VAL' \n",
    "units = 'INCHES' \n",
    "\n",
    "\n",
    "## Filenames and paths:\n",
    "Temporal_Distribution_Data_Map = 'Temporal_Distribution_Data_Map.json'\n",
    "Temporal_Quartile_Ranks = 'Temporal_Quartile_Ranks.xlsx'\n",
    "CN_Distribution = 'NEH630_Table_10_1.json'\n",
    "temporal_dir = datarepository_dir/'Temporal_Distributions'\n",
    "dssutil = 'DSSUTL.EXE'\n",
    "\n",
    "\n",
    "## Options:\n",
    "adjust_CN_less24 = False # Set to True to adjust the curve number when the storm duration is less than 24 hours\n",
    "remove_intermediates = True # Remove intermediate metadata files that are saved to the metadata json\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area averaged precipitation frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Expected Value</th>\n",
       "      <th>Lower (90%)</th>\n",
       "      <th>Upper (90%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.157397</td>\n",
       "      <td>2.876503</td>\n",
       "      <td>3.507128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.057966</td>\n",
       "      <td>3.691363</td>\n",
       "      <td>4.502486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Expected Value  Lower (90%)  Upper (90%)\n",
       "Tr                                          \n",
       "2         3.157397     2.876503     3.507128\n",
       "5         4.057966     3.691363     4.502486"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_precip = get_input_data(precip_table_dir, duration, lower_limit, display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal distribution data map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': {'q1': [0, 9], 'q2': [11, 20], 'q3': [22, 31], 'q4': [33, 42]}, 'skiprows': 12}\n"
     ]
    }
   ],
   "source": [
    "qmap = get_temporal_map(datarepository_dir, Temporal_Distribution_Data_Map, volume, region, duration, display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precipitation temporal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>8.3</th>\n",
       "      <th>16.7</th>\n",
       "      <th>25.0</th>\n",
       "      <th>33.3</th>\n",
       "      <th>41.7</th>\n",
       "      <th>50.0</th>\n",
       "      <th>58.3</th>\n",
       "      <th>66.7</th>\n",
       "      <th>75.0</th>\n",
       "      <th>83.3</th>\n",
       "      <th>91.7</th>\n",
       "      <th>100.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent of duration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>0</td>\n",
       "      <td>55.1</td>\n",
       "      <td>85.7</td>\n",
       "      <td>96.2</td>\n",
       "      <td>99.1</td>\n",
       "      <td>99.9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>0</td>\n",
       "      <td>41.8</td>\n",
       "      <td>71.0</td>\n",
       "      <td>86.7</td>\n",
       "      <td>94.4</td>\n",
       "      <td>97.9</td>\n",
       "      <td>99.3</td>\n",
       "      <td>99.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0.0   8.3  16.7  25.0  33.3  41.7   50.0   58.3   66.7  \\\n",
       "percent of duration                                                          \n",
       "10%                   0  55.1  85.7  96.2  99.1  99.9  100.0  100.0  100.0   \n",
       "20%                   0  41.8  71.0  86.7  94.4  97.9   99.3   99.8  100.0   \n",
       "\n",
       "                      75.0   83.3   91.7  100.0  \n",
       "percent of duration                              \n",
       "10%                  100.0  100.0  100.0  100.0  \n",
       "20%                  100.0  100.0  100.0  100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_temporals = get_temporals(temporal_dir, volume, region, duration, qmap, display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal quartile ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36, 0.26, 0.23, 0.15]\n"
     ]
    }
   ],
   "source": [
    "qrank = get_quartile_rank(datarepository_dir, Temporal_Quartile_Ranks, volume, region, duration, display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duration weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25371923552866765\n"
     ]
    }
   ],
   "source": [
    "duration_weight = get_duration_weight(datarepository_dir, Temporal_Quartile_Ranks, volume, region, duration, display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curve number distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dry': 67, 'Wet': 93}\n"
     ]
    }
   ],
   "source": [
    "arc_data = get_CN_distribution(datarepository_dir, CN_Distribution, CN, display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolate the precipitation data for 2000 and 3000 year events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Expected Value</th>\n",
       "      <th>Lower (90%)</th>\n",
       "      <th>Upper (90%)</th>\n",
       "      <th>Ann. Exc. Prob.</th>\n",
       "      <th>ARI</th>\n",
       "      <th>Log10_ARI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tr</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.157397</td>\n",
       "      <td>2.876503</td>\n",
       "      <td>3.507128</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.442695</td>\n",
       "      <td>0.366513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.057966</td>\n",
       "      <td>3.691363</td>\n",
       "      <td>4.502486</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>4.481420</td>\n",
       "      <td>1.499940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.851486</td>\n",
       "      <td>4.392726</td>\n",
       "      <td>5.365212</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>9.491222</td>\n",
       "      <td>2.250367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.069994</td>\n",
       "      <td>5.449989</td>\n",
       "      <td>6.672994</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>24.496598</td>\n",
       "      <td>3.198534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.151799</td>\n",
       "      <td>6.371184</td>\n",
       "      <td>7.828531</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>49.498316</td>\n",
       "      <td>3.901939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>8.373408</td>\n",
       "      <td>7.391402</td>\n",
       "      <td>9.138771</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>99.499162</td>\n",
       "      <td>4.600149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>9.766151</td>\n",
       "      <td>8.526531</td>\n",
       "      <td>10.630844</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>199.499582</td>\n",
       "      <td>5.295812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>11.904352</td>\n",
       "      <td>10.219179</td>\n",
       "      <td>12.912581</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>499.499833</td>\n",
       "      <td>6.213607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>13.782117</td>\n",
       "      <td>11.668648</td>\n",
       "      <td>14.920983</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>999.499917</td>\n",
       "      <td>6.907255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>15.956076</td>\n",
       "      <td>13.323706</td>\n",
       "      <td>17.241768</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1999.499958</td>\n",
       "      <td>7.600652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>17.382363</td>\n",
       "      <td>14.397836</td>\n",
       "      <td>18.762115</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>2999.499972</td>\n",
       "      <td>8.006201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Expected Value  Lower (90%)  Upper (90%)  Ann. Exc. Prob.          ARI  \\\n",
       "Tr                                                                             \n",
       "2           3.157397     2.876503     3.507128         0.500000     1.442695   \n",
       "5           4.057966     3.691363     4.502486         0.200000     4.481420   \n",
       "10          4.851486     4.392726     5.365212         0.100000     9.491222   \n",
       "25          6.069994     5.449989     6.672994         0.040000    24.496598   \n",
       "50          7.151799     6.371184     7.828531         0.020000    49.498316   \n",
       "100         8.373408     7.391402     9.138771         0.010000    99.499162   \n",
       "200         9.766151     8.526531    10.630844         0.005000   199.499582   \n",
       "500        11.904352    10.219179    12.912581         0.002000   499.499833   \n",
       "1000       13.782117    11.668648    14.920983         0.001000   999.499917   \n",
       "2000       15.956076    13.323706    17.241768         0.000500  1999.499958   \n",
       "3000       17.382363    14.397836    18.762115         0.000333  2999.499972   \n",
       "\n",
       "      Log10_ARI  \n",
       "Tr               \n",
       "2      0.366513  \n",
       "5      1.499940  \n",
       "10     2.250367  \n",
       "25     3.198534  \n",
       "50     3.901939  \n",
       "100    4.600149  \n",
       "200    5.295812  \n",
       "500    6.213607  \n",
       "1000   6.907255  \n",
       "2000   7.600652  \n",
       "3000   8.006201  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "raw_precip = extrap_add_ari(raw_precip, display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Randomly Select Recurrance Intervals and Precipitation Amounts:\n",
    "\n",
    "For each quartile of the precipitation temporal distributions:\n",
    "- Randomly select precipitation recurrance intervals\n",
    "- Calculate the expected value, lower confidence limit, and upper confidence limit for each recurrance interval\n",
    "- Calculate the standard devation of the log-normal distribution using the lower/upper confidence limits at each recurrance interval\n",
    "- Randomly select a precipitation amount from the calibrated log-normal distributuion at each recurrance interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tables, random_precip_tables, outfiles = [], [], []\n",
    "\n",
    "quartiles = range(1, nquartiles+1)  \n",
    "\n",
    "for i, quartile in enumerate(quartiles):\n",
    "    start = time.time()\n",
    "    r_events = generate_random_samples(int(n_events*qrank[i]), seed, display_print = False)\n",
    "    use_r_events = Truncate_Random_Events(r_events)\n",
    "    data_table = events_table_random(raw_precip, use_r_events)\n",
    "    data_table['Quartile'] = quartile\n",
    "    fitted_precip = find_optimal_curve_std(data_table)\n",
    "    filename = 'Rand_Precip_Q{0}_{1}_Dur{2}_tempE{3}_convE{4}_volE{5}_Se{6}.csv'.format(quartile, AOI, duration, tempEpsilon, convEpsilon, volEpsilon, seed)\n",
    "    outfiles.append(filename)\n",
    "    random_precip_table = RandomizeData(fitted_precip, fitted_precip.shape[0], outputs_dir, filename, duration, seed, plot = display_plots, display_print = False)\n",
    "    random_precip_table['Quartile'] = quartile\n",
    "    data_tables.append(data_table)\n",
    "    random_precip_tables.append(random_precip_table)\n",
    "    if display_print: print('Data tables for Quartile {} computed in {} Seconds'.format(quartile, str(round((time.time()-start),3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine & format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The randomized precipitation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_precip_table = join_rdata_tables(random_precip_tables, 'Precip', display_print)\n",
    "\n",
    "n_rand_events = random_precip_table.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = join_rdata_tables(data_tables, 'MetaData', display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Randomly Select Temporal Distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat and plot the temporal distribution data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_group = get_quartiles(raw_temporals, duration, qrank, qmap, volume, region, plot = display_plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select the decile and plot:\n",
    "Note: The decile number is randomly selected for each event, and each event is already associated with a quartile, therefore, given the quartile and the decile, the specific temporal distribution is assigned for each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporals = map_quartiles_deciles(n_rand_events, seed, plot = display_plots, display_print = display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Randomly Select Curve Numbers:\n",
    "- Add the lower and upper values of the specified curve number to a table\n",
    "- Calculate the standard devation of the log-normal distribution for the curve number using the lower and upper values.\n",
    "- Randomly select a curve number from the calibrated log-normal distributuion at each recurrance interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CN = prep_cn_table(CN, arc_data)  \n",
    "\n",
    "fitted_cn = find_optimal_curve_std(df_CN, 'Lower', 'Upper')\n",
    "\n",
    "filename = \"Rand_CN_{0}_{1}_Dur{2}_tempE{3}_convE{4}_volE{5}_Se{6}.csv\".format(quartile, AOI, duration, tempEpsilon, convEpsilon, volEpsilon, seed)\n",
    "outfiles.append(filename)\n",
    "\n",
    "random_cns = RandomizeData(fitted_cn, n_rand_events, outputs_dir, filename, duration, seed=seed, variable='CN', lower='Lower', upper='Upper', display_print = display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Calculate Excess Rainfall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_precip, cum_excess, incr_excess, events_metadata =  populate_event_precip_data(random_cns, temporals, random_precip_table, data_table, curve_group, duration, adjust_CN_less24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = plot_rainfall_and_excess(final_precip, cum_excess, duration, iplot = papermill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Combine Temporal Curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform first convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binData = bin_sorting_dev(incr_excess, nbins, display_print = False)\n",
    "\n",
    "convolution_dic = hydromet_conv.main(binData, incr_excess, tempEpsilon, convEpsilon, volEpsilon, test_stat_threshold, display_print = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract individual objects from the convolution dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penult_curves = convolution_dic['penult_curves']\n",
    "\n",
    "penult_groups = convolution_dic['penult_groups']\n",
    "\n",
    "penult_tests = convolution_dic['penult_tests']\n",
    "\n",
    "midbin_curve_df = convolution_dic['midbin_curve_df']\n",
    "\n",
    "midbin_group = convolution_dic['midbin_group']\n",
    "\n",
    "midbin_curves = convolution_dic['midbin_curves']\n",
    "\n",
    "#### Uncomment to plot the middle bin's curve groups:\n",
    "# y_max = midbin_curve_df.max().max()\n",
    "# if display_plots: plot_curve_groups(midbin_group, midbin_curves, midbin_curve_df, y_max, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC the convolution results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_curves_initial = incr_excess.shape[1]\n",
    "n_curves_penult = len(dic_to_list(penult_groups))\n",
    "assert(n_curves_initial == n_curves_penult, '# of events not conserved, missing {} events'.format(n_curves_initial-n_curves_penult))\n",
    "\n",
    "min_test_statistic = min(dic_to_list(penult_tests))\n",
    "if display_print: print('Minimum Test Statistic:', min_test_statistic)\n",
    "\n",
    "n_meancurves = penult_curves.shape[1]\n",
    "penult_reduction = (1-n_meancurves/n_curves_initial)*100.0\n",
    "\n",
    "if display_print: print(\"Number of curves reduced by {0}% or {1} curves out of {2} remaining\".format(np.round(penult_reduction, 2), n_meancurves, n_curves_initial))\n",
    "\n",
    "if display_print: print(display(penult_curves.head(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the combined temporal curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max = penult_curves.max().max()\n",
    "\n",
    "f1 = plot_grouped_curves(penult_curves, y_max, iplot = papermill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform final convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "adj_tempEpsilon2 = convert_tempEpsilon(tempEpsilon2, incr_excess)\n",
    "\n",
    "curv_df = prep_data_for_convolution(penult_curves, adj_tempEpsilon2)\n",
    "\n",
    "test_dic, test_values = conv_ts(curv_df, convEpsilon, volEpsilon)\n",
    "\n",
    "events = list(curv_df.columns)                                                                         \n",
    "\n",
    "all_groups = group_curves(test_dic, test_values, events, test_stat_threshold)    \n",
    "\n",
    "upd_curv = calc_mean_curves(all_groups, curv_df) \n",
    "\n",
    "final_curves = calc_mean_curves(all_groups, penult_curves) \n",
    "\n",
    "final_groups = map_curve_groups(penult_groups, all_groups, ungroup = True)\n",
    "\n",
    "final_tests = final_test_stat(final_groups, final_curves, incr_excess, convEpsilon, volEpsilon)\n",
    "\n",
    "if display_print: print('Processed {0} curves in {1} Minutes'.format(curv_df.shape[1], round(time.time()-start)/60, 3))\n",
    "    \n",
    "#### Uncomment to plot the curve groups:\n",
    "#y_max = incr_excess.max().max()\n",
    "#if display_plots: plot_curve_groups(final_groups, final_curves, incr_excess, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC the convolution results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_curves_final = len(dic_to_list(final_groups))\n",
    "assert(n_curves_initial == n_curves_final, '# of events not conserved, missing {} events'.format(n_curves_initial-n_curves_final))\n",
    "\n",
    "min_test_statistic = min(dic_to_list(final_tests))\n",
    "if display_print: print('Minimum Test Statistic:', min_test_statistic)\n",
    "\n",
    "n_meancurves = final_curves.shape[1]\n",
    "final_reduction = (1-n_meancurves/n_curves_initial)*100.0\n",
    "\n",
    "sb.glue('final_reduction_lst', [final_reduction, n_meancurves, n_curves_initial])\n",
    "\n",
    "if display_print: print(\"Number of curves reduced by {0}% or {1} curves out of {2} remaining\".format(np.round(final_reduction, 2), n_meancurves, n_curves_initial))\n",
    "\n",
    "if display_print: print(display(final_curves.head(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the combined temporal curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max = final_curves.max().max()\n",
    "\n",
    "f2 = plot_grouped_curves(final_curves, y_max, iplot = papermill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Calculate the Combined Weight and Rename the Groups:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_weight = Calc_Group_Weight(final_groups, duration_weight, display_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the groups according to their weight (largest to smallest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = Rename_Final_Groups(curve_weight, duration)\n",
    "\n",
    "renamed_groups = {}\n",
    "renamed_tests = {}\n",
    "renamed_weights = {}\n",
    "\n",
    "for k, v in rename_map.items():\n",
    "    renamed_groups[v] = final_groups[k]\n",
    "    renamed_tests[v] = final_tests[k]\n",
    "    renamed_weights[v] = curve_weight[k]\n",
    "\n",
    "renamed_curves = final_curves.rename(index=str, columns=rename_map).copy()\n",
    "renamed_curves = renamed_curves.reindex(sorted(renamed_curves.columns), axis=1)\n",
    "renamed_curves.head(2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the weights dictionary to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed_weights = pd.DataFrame(data = {'Weight':list(renamed_weights.values())}, index = list(renamed_weights.keys()))\n",
    "df_renamed_weights.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Save the Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined excess rainfall curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_curves.to_csv(outputs_dir/'Excess_Rainfall_{0}_Dur{1}_tempE{2}_convE{3}_volE{4}.csv'.format(AOI, duration, tempEpsilon, convEpsilon, volEpsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To DSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_dss:\n",
    "    AOI = AOI.replace(' ', '_')\n",
    "    \n",
    "    scen_name = '{0}_Dur{1}'.format(AOI, duration)\n",
    "    \n",
    "    dss_filename = 'Excess_Rainfall_'+scen_name+'_tempE{0}_convE{1}_volE{2}'.format(tempEpsilon, convEpsilon, volEpsilon)\n",
    "    \n",
    "    tstep_dic = determine_tstep_units(incr_excess)\n",
    "    tstep, tstep_units = list(tstep_dic.keys())[0], list(tstep_dic.values())[0]\n",
    "\n",
    "    dss_map(outputs_dir, variable, tstep, tstep_units, units, data_type)\n",
    "    \n",
    "    excess_df_to_input(outputs_dir, renamed_curves, tstep, tstep_units, scen_name)\n",
    "    \n",
    "    make_dss_file(outputs_dir, bin_dir, dssutil, dss_filename, remove_temp_files = True, display_print = display_print)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed_weights.to_csv(outputs_dir/'Weights_{0}_Dur{1}_tempE{2}_convE{3}_volE{4}.csv'.format(AOI, duration, tempEpsilon, convEpsilon, volEpsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {}\n",
    "\n",
    "metadata['groups'] = dic_key_to_str(renamed_groups)\n",
    "metadata['test_stat'] = dic_key_to_str(renamed_tests)\n",
    "metadata['precip'] = final_precip.to_dict()\n",
    "metadata['cum_excess'] = cum_excess.to_dict()\n",
    "metadata['incr_excess'] = incr_excess.to_dict()\n",
    "metadata['events_metadata'] = extract_event_metadata(outfiles, outputs_dir, remove_intermediates)\n",
    "\n",
    "with open(os.path.join(str(outputs_dir.resolve()),'Metadata_{0}_Dur{1}_tempE{2}_convE{3}_volE{4}.json'.format(AOI, duration, tempEpsilon, convEpsilon, volEpsilon)), 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "    \n",
    "if display_print: print('Runtime: {} Minutes'.format(round(time.time()-start_time)/60, 3))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
